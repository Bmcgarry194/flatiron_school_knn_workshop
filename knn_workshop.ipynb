{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors: Predicting King County Housing Prices\n",
    "\n",
    "<img src=\"neighbors-talking-over-fence-min.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you predict the price of a house that is about to go on sale?\n",
    "\n",
    "<img src='For-sale-sign.jpg' alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar houses should be similar in price\n",
    "\n",
    "* Square footage\n",
    "* Number of floors\n",
    "* Location\n",
    "\n",
    "\n",
    "## Distance as a measure of similarity\n",
    "\n",
    "How 'far away' are houses from each other given all of their features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is K-Nearest Neighbors?\n",
    "\n",
    "**_K-Nearest Neighbors_** (or KNN, for short) is a supervised learning algorithm that can be used for both **_Classification_** and **_Regression_** tasks. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between 2 points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a **_Supervised Learning Algorithm_**, we must also have the labels for each point in our dataset, or else we can't use this algorithm for prediction.\n",
    "\n",
    "## Fitting the Model\n",
    "\n",
    "KNN is unique compared to other algorithms in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the 'fit' step, KNN just stores all the training data and corresponding values. No distances are calculated at this point. \n",
    "\n",
    "## Making Predictions with K\n",
    "\n",
    "All the magic happens during the 'predict' step. During this step, KNN takes a point that we want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the `K` closest points, or **_Neighbors_**, and examines the values of each. You can think of each of the K-closest points getting a 'vote' about the predicted value. Often times the mean of all the values is taken to make a prediction about the new point.\n",
    "\n",
    "In the following animation, K=3.\n",
    "\n",
    "<img src='knn.gif'>\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "As we explored in a previous lesson, there are different **_distance metrics_** when using KNN. For KNN, we can use **_Manhattan_**, **_Euclidean_**, or **_Minkowski Distance_**--from an algorithmic standpoint, it doesn't matter which! However, it should be noted that from a practical standpoint, these can affect our results and our overall model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = pd.read_csv('data/kc-house-data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Transforming\n",
    "\n",
    "Sklearn is one of the most popular ML libraries for python which gives us access to a wealth of different algorthims. All of these algorthims follow the same API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = model_object()\n",
    "\n",
    "model.fit()\n",
    "\n",
    "model.predict()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own implementation of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test, k=3):\n",
    "        \n",
    "        predictions = np.zeros(X_test.shape[0])\n",
    "        \n",
    "        for i, point in enumerate(X_test):\n",
    "            distances = self._get_distances(point)\n",
    "            k_nearest = self._get_k_nearest(distances, k)\n",
    "            prediction = self._get_predicted_value(k_nearest)\n",
    "            predictions[i] = prediction\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    #helper functions\n",
    "    def _get_distances(self, x):\n",
    "        '''Take an single point and return an array of distances to every point in our dataset'''\n",
    "        distances = np.zeros(self.X_train.shape[0])\n",
    "        for i, point in enumerate(self.X_train):\n",
    "            distances[i] = euc(x, point)\n",
    "        return distances\n",
    "    \n",
    "    def _get_k_nearest(self, distances, k):\n",
    "        '''Take in the an array of distances and return the indices of the k nearest points'''\n",
    "        nearest = np.argsort(distances)[:k]\n",
    "        return nearest\n",
    "    \n",
    "    def _get_predicted_value(self, k_nearest):\n",
    "        '''Takes in the indices of the k nearest points and returns the mean of their target values'''\n",
    "        return np.mean(self.y_train[k_nearest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit our predictions to the middle 80% of our dataset\n",
    "\n",
    "It is easier to make predictions where the data is most dense but doing this means that any predictions made outside of the range of values we are training on will be highly suspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10 = np.percentile(house_data['price'], 10)\n",
    "top_10 = np.percentile(house_data['price'], 90)\n",
    "\n",
    "house_data = house_data[(house_data['price'] > bottom_5) & (house_data['price'] < top_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(house_data['price'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sqft_living', 'lat', 'long']\n",
    "\n",
    "X = house_data[features]\n",
    "y = house_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to scale our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this so slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will run for a long time\n",
    "preds = knn.predict(X_test_scaled, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use Sklearn's KNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "sk_preds = nn.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, sk_preds))\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the optimal number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 30)\n",
    "\n",
    "test_errors = np.zeros(len(list(ks)))\n",
    "\n",
    "for i, k in enumerate(ks):\n",
    "    \n",
    "    nn = KNeighborsRegressor(n_neighbors=k, n_jobs=-1)\n",
    "\n",
    "    nn.fit(X_train_scaled, y_train)\n",
    "    preds = nn.predict(X_test_scaled)\n",
    "    \n",
    "    test_errors[i] = np.sqrt(mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(list(ks), test_errors)\n",
    "ax.axvline(list(ks)[np.argmin(test_errors)], linestyle='--', color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = list(ks)[np.argmin(test_errors)]\n",
    "\n",
    "optimal_error = np.min(test_errors)\n",
    "\n",
    "print(f'Optimal number of Neighbors: {optimal_k} Root Mean Squared Error: {optimal_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
